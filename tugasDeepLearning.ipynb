{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azarizz/deepLearning/blob/main/tugasDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rae54ZjRnAg6",
        "outputId": "a1b35b71-f396-4267-c830-f5b5c1722d92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from collections import deque\n",
        "\n",
        "class PoetryGeneratorFromFile:\n",
        "    def __init__(self, max_sequence_len=100, max_words=5000):\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.max_words = max_words\n",
        "        self.tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "        self.model = None\n",
        "\n",
        "    def load_dataset(self, filename):\n",
        "        \"\"\"\n",
        "        Membaca dataset dari file txt\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(filename, 'r', encoding='utf-8') as file:\n",
        "                poems = file.read().split('\\n\\n')  # Memisahkan puisi berdasarkan baris kosong\n",
        "\n",
        "            # Membersihkan data\n",
        "            poems = [poem.strip() for poem in poems if poem.strip()]\n",
        "            return poems\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File {filename} tidak ditemukan!\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error membaca file: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def prepare_data(self, poems):\n",
        "        \"\"\"\n",
        "        Mempersiapkan data puisi untuk training\n",
        "        \"\"\"\n",
        "        # Tokenisasi teks\n",
        "        self.tokenizer.fit_on_texts(poems)\n",
        "        total_words = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # Membuat sequences\n",
        "        input_sequences = []\n",
        "        for poem in poems:\n",
        "            token_list = self.tokenizer.texts_to_sequences([poem])[0]\n",
        "            for i in range(1, len(token_list)):\n",
        "                n_gram_sequence = token_list[:i+1]\n",
        "                input_sequences.append(n_gram_sequence)\n",
        "\n",
        "        # Padding sequences\n",
        "        padded_sequences = pad_sequences(input_sequences,\n",
        "                                      maxlen=self.max_sequence_len,\n",
        "                                      padding='pre')\n",
        "\n",
        "        # Membuat input dan target\n",
        "        X = padded_sequences[:, :-1]\n",
        "        y = padded_sequences[:, -1]\n",
        "        y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "        return X, y, total_words\n",
        "\n",
        "    def build_model(self, total_words, embedding_dim=100):\n",
        "        \"\"\"\n",
        "        Membangun model LSTM\n",
        "        \"\"\"\n",
        "        self.model = Sequential([\n",
        "            Embedding(total_words, embedding_dim,\n",
        "                     input_length=self.max_sequence_len-1),\n",
        "            LSTM(256, return_sequences=True),\n",
        "            LSTM(256),\n",
        "            Dense(256, activation='relu'),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(total_words, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        self.model.compile(loss='categorical_crossentropy',\n",
        "                          optimizer='adam',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "    def train(self, X, y, epochs=100, batch_size=32):\n",
        "        \"\"\"\n",
        "        Melatih model\n",
        "        \"\"\"\n",
        "        history = self.model.fit(X, y,\n",
        "                               epochs=epochs,\n",
        "                               batch_size=batch_size,\n",
        "                               validation_split=0.1)\n",
        "        return history\n",
        "\n",
        "    def sample_with_temperature(self, predictions, temperature=0.7):\n",
        "        \"\"\"\n",
        "        Menggunakan temperature sampling untuk meningkatkan kreativitas\n",
        "        \"\"\"\n",
        "        predictions = np.asarray(predictions).astype('float64')\n",
        "        predictions = np.log(predictions) / temperature\n",
        "        exp_predictions = np.exp(predictions)\n",
        "        predictions = exp_predictions / np.sum(exp_predictions)\n",
        "        probas = np.random.multinomial(1, predictions, 1)\n",
        "        return np.argmax(probas)\n",
        "\n",
        "    def generate_poem(self, seed_text, next_words=50, temperature=0.7):\n",
        "        \"\"\"\n",
        "        Menghasilkan puisi dengan mencegah pengulangan\n",
        "        \"\"\"\n",
        "        recent_words = deque(maxlen=5)\n",
        "        result = seed_text\n",
        "\n",
        "        for _ in range(next_words):\n",
        "            token_list = self.tokenizer.texts_to_sequences([result])[0]\n",
        "            token_list = pad_sequences([token_list],\n",
        "                                     maxlen=self.max_sequence_len-1,\n",
        "                                     padding='pre')\n",
        "\n",
        "            predicted = self.model.predict(token_list, verbose=0)[0]\n",
        "            next_index = self.sample_with_temperature(predicted, temperature)\n",
        "\n",
        "            output_word = \"\"\n",
        "            for word, index in self.tokenizer.word_index.items():\n",
        "                if index == next_index:\n",
        "                    output_word = word\n",
        "                    break\n",
        "\n",
        "            if output_word in recent_words:\n",
        "                continue\n",
        "\n",
        "            recent_words.append(output_word)\n",
        "            result += \" \" + output_word\n",
        "\n",
        "            if len(recent_words) % np.random.randint(6, 9) == 0:\n",
        "                result += \"\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "# Contoh penggunaan\n",
        "if __name__ == \"__main__\":\n",
        "    # Inisialisasi generator\n",
        "    generator = PoetryGeneratorFromFile()\n",
        "\n",
        "    # Baca dataset dari file\n",
        "    poems = generator.load_dataset('puisi.txt')\n",
        "\n",
        "    if poems:\n",
        "        print(f\"Berhasil memuat {len(poems)} puisi dari file\")\n",
        "\n",
        "        # Persiapkan data\n",
        "        X, y, total_words = generator.prepare_data(poems)\n",
        "\n",
        "        # Bangun dan latih model\n",
        "        generator.build_model(total_words)\n",
        "        history = generator.train(X, y, epochs=100)\n",
        "\n",
        "        # Generate puisi baru\n",
        "        seed = \"Aku adalah\"\n",
        "        generated_poem = generator.generate_poem(seed, temperature=0.7)\n",
        "        print(\"\\nPuisi yang dihasilkan:\")\n",
        "        print(generated_poem)\n",
        "    else:\n",
        "        print(\"Tidak dapat melanjutkan karena error dalam membaca file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_d9wrq-nGfE",
        "outputId": "23ab6ba2-6eaa-4da6-cc41-0cb78156eae6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File puisi.txt tidak ditemukan!\n",
            "Tidak dapat melanjutkan karena error dalam membaca file\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}